{
  "cells": [
    {
      "cell_type": "raw",
      "id": "62727aaa-bcff-4087-891c-e539f824ee1f",
      "metadata": {
        "id": "62727aaa-bcff-4087-891c-e539f824ee1f"
      },
      "source": [
        "---\n",
        "sidebar_label: Wikipedia\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62a16c1-10de-4f99-b392-c4ad2e6123a1",
      "metadata": {
        "id": "d62a16c1-10de-4f99-b392-c4ad2e6123a1"
      },
      "source": [
        "# WikipediaRetriever\n",
        "\n",
        "## Overview\n",
        ">[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.\n",
        "\n",
        "This notebook shows how to retrieve wiki pages from `wikipedia.org` into the [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that is used downstream.\n",
        "\n",
        "### Integration details\n",
        "\n",
        "import {ItemTable} from \"@theme/FeatureTables\";\n",
        "\n",
        "<ItemTable category=\"external_retrievers\" item=\"WikipediaRetriever\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7d377c-168b-40e8-bd61-af6a4fb1b44f",
      "metadata": {
        "id": "eb7d377c-168b-40e8-bd61-af6a4fb1b44f"
      },
      "source": [
        "## Setup\n",
        "To enable automated tracing of individual tools, set your [LangSmith](https://docs.smith.langchain.com/) API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbc6013-2617-4f7e-9d8b-7453d09315c0",
      "metadata": {
        "id": "1bbc6013-2617-4f7e-9d8b-7453d09315c0"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51489529-5dcd-4b86-bda6-de0a39d8ffd1",
      "metadata": {
        "id": "51489529-5dcd-4b86-bda6-de0a39d8ffd1"
      },
      "source": [
        "### Installation\n",
        "\n",
        "The integration lives in the `langchain-community` package. We also need to install the `wikipedia` python package itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a737220",
      "metadata": {
        "tags": [],
        "id": "1a737220"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain_community wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae622ac6-d18a-4754-a4bd-d30a078c19b5",
      "metadata": {
        "id": "ae622ac6-d18a-4754-a4bd-d30a078c19b5"
      },
      "source": [
        "## Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c15470b-a16b-4e0d-bc6a-6998bafbb5a4",
      "metadata": {
        "id": "6c15470b-a16b-4e0d-bc6a-6998bafbb5a4"
      },
      "source": [
        "Now we can instantiate our retriever:\n",
        "\n",
        "`WikipediaRetriever` parameters include:\n",
        "- optional `lang`: default=\"en\". Use it to search in a specific language part of Wikipedia\n",
        "- optional `load_max_docs`: default=100. Use it to limit number of downloaded documents. It takes time to download all 100 documents, so use a small number for experiments. There is a hard limit of 300 for now.\n",
        "- optional `load_all_available_meta`: default=False. By default only the most important fields downloaded: `Published` (date when document was published/last updated), `title`, `Summary`. If True, other fields also downloaded.\n",
        "\n",
        "`get_relevant_documents()` has one argument, `query`: free text which used to find documents in Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78f0cd0-ffea-4fe3-9d1d-54639c4ef1ff",
      "metadata": {
        "id": "b78f0cd0-ffea-4fe3-9d1d-54639c4ef1ff"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "\n",
        "retriever = WikipediaRetriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12aead36-7b97-4d9c-82e7-ec644a3127f9",
      "metadata": {
        "id": "12aead36-7b97-4d9c-82e7-ec644a3127f9"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a76605-6b1e-44bf-b8a2-7d48119290c4",
      "metadata": {
        "id": "54a76605-6b1e-44bf-b8a2-7d48119290c4"
      },
      "outputs": [],
      "source": [
        "docs = retriever.invoke(\"TOKYO GHOUL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ada2b7-3507-4dcb-9982-5f8f4e97a2e1",
      "metadata": {
        "id": "65ada2b7-3507-4dcb-9982-5f8f4e97a2e1",
        "outputId": "12553140-84be-4e11-fa25-56090244a961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokyo Ghoul (Japanese: 東京喰種（トーキョーグール）, Hepburn: Tōkyō Gūru) is a Japanese dark fantasy manga series written and illustrated by Sui Ishida. It was serialized in Shueisha's seinen manga magazine Weekly Young Jump from September 2011 to September 2014, with its chapters collected in 14 tankōbon volumes. The story is set in an alternate version of Tokyo where humans coexist with ghouls, beings who loo\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[:400])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def index_encoded_data(self, index, embedding_files, indexing_batch_size):\n",
        "    allids = []\n",
        "    allembeddings = np.array([])\n",
        "    for i, file_path in enumerate(embedding_files):\n",
        "        print(f\"Loading file {file_path}\")\n",
        "        with open(file_path, \"rb\") as fin:\n",
        "            ids, embeddings = pickle.load(fin)\n",
        "\n",
        "        allembeddings = np.vstack((allembeddings, embeddings)) if allembeddings.size else embeddings\n",
        "        allids.extend(ids)\n",
        "        while allembeddings.shape[0] > indexing_batch_size:\n",
        "            allembeddings, allids = self.add_embeddings(index, allembeddings, allids, indexing_batch_size)\n",
        "\n",
        "    while allembeddings.shape[0] > 0:\n",
        "        allembeddings, allids = self.add_embeddings(index, allembeddings, allids, indexing_batch_size)\n",
        "\n",
        "    print(\"Data indexing completed.\")\n",
        "\n",
        "\n",
        "def add_embeddings(self, index, embeddings, ids, indexing_batch_size):\n",
        "    end_idx = min(indexing_batch_size, embeddings.shape[0])\n",
        "    ids_toadd = ids[:end_idx]\n",
        "    embeddings_toadd = embeddings[:end_idx]\n",
        "    ids = ids[end_idx:]\n",
        "    embeddings = embeddings[end_idx:]\n",
        "    index.index_data(ids_toadd, embeddings_toadd)\n",
        "    return embeddings, ids"
      ],
      "metadata": {
        "id": "EgmrgV9R5f2H"
      },
      "id": "EgmrgV9R5f2H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install faiss-gpu\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence_transformers\n",
        "import faiss\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "TQiWJxi35FOX"
      },
      "id": "TQiWJxi35FOX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_folder = \"/kaggle/input/wikipedia-faiss-index\"\n",
        "\n",
        "for idx, indexname in enumerate(os.listdir(index_folder)):\n",
        "    index = faiss.read_index(os.path.join(index_folder, indexname))\n",
        "    faiss.write_index(index, os.path.join(\"/kaggle/working/\", indexname))\n",
        "    print(f\"Successfullt move the {indexname} from Input to Output\")"
      ],
      "metadata": {
        "id": "f6BJ2j1A5Jhj"
      },
      "id": "f6BJ2j1A5Jhj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"thenlper/gte-small\"\n",
        "sentence_transformer = SentenceTransformer(model_name)\n",
        "parquet_folder = \"/kaggle/input/wikipedia-20230701\"\n",
        "\n",
        "file_names = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'number', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "for idx, filename in enumerate(file_names):\n",
        "    if (idx + 1) >= 22:\n",
        "        document_embeddings = []\n",
        "\n",
        "        print(f\"Processing file_id: {idx + 1} - file_name: {filename}.parquet ......\")\n",
        "\n",
        "        parquet_path = os.path.join(parquet_folder, f\"{filename}.parquet\")\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "\n",
        "        print(df.columns)\n",
        "        print(\"Sample text: \", df.iloc[0][\"text\"])\n",
        "\n",
        "        sentences = df[\"text\"].tolist()\n",
        "        embeddings = sentence_transformer.encode(sentences, normalize_embeddings=True)\n",
        "        document_embeddings.extend(embeddings)\n",
        "\n",
        "        del df\n",
        "\n",
        "        document_embeddings = np.array(document_embeddings).astype(\"float32\")\n",
        "        index = faiss.IndexFlatIP(document_embeddings.shape[1])\n",
        "        index.add(document_embeddings)\n",
        "        faiss_index_path = f\"/kaggle/working/wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        faiss.write_index(index, faiss_index_path)\n",
        "\n",
        "\n",
        "        print(f\"Faiss index saved to '{faiss_index_path}'\")"
      ],
      "metadata": {
        "id": "yb1AbLEq5Ksb"
      },
      "id": "yb1AbLEq5Ksb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_folder1 = \"/kaggle/input/wikipedia-faiss-index\"\n",
        "index_folder2 = \"/kaggle/input/wikipedia-faiss-index\"\n",
        "\n",
        "file_names = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'number', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "merged_index = faiss.IndexFlatL2(384)\n",
        "for idx, filename in enumerate(file_names):\n",
        "    if (idx + 1) >= 7:\n",
        "        break\n",
        "\n",
        "    if (idx + 1) >= 12 and (idx + 1) <= 20:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder2, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    else:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder1, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    if (idx + 1) == 6:\n",
        "        merged_index_path = \"/kaggle/working/merged_1.index\"\n",
        "        faiss.write_index(merged_index, merged_index_path)\n",
        "\n",
        "        print(f\"Merged index saved to '{merged_index_path}'\")\n",
        "\n",
        "        del merged_index\n",
        "\n",
        "\n",
        "merged_index = faiss.IndexFlatL2(384)\n",
        "for idx, filename in enumerate(file_names):\n",
        "    if (idx + 1) <= 6:\n",
        "        continue\n",
        "\n",
        "    if (idx + 1) == 13:\n",
        "        break\n",
        "\n",
        "    if (idx + 1) >= 12 and (idx + 1) <= 20:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder2, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    else:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder1, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    if (idx + 1) == 12:\n",
        "        merged_index_path = \"/kaggle/working/merged_2.index\"\n",
        "        faiss.write_index(merged_index, merged_index_path)\n",
        "\n",
        "        print(f\"Merged index saved to '{merged_index_path}'\")\n",
        "\n",
        "        del merged_index\n",
        "\n",
        "\n",
        "merged_index = faiss.IndexFlatL2(384)\n",
        "for idx, filename in enumerate(file_names):\n",
        "    if (idx + 1) <= 12:\n",
        "        continue\n",
        "\n",
        "    if (idx + 1) == 20:\n",
        "        break\n",
        "\n",
        "    if (idx + 1) >= 12 and (idx + 1) <= 20:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder2, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    else:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder1, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    if (idx + 1) == 19:\n",
        "        merged_index_path = \"/kaggle/working/merged_3.index\"\n",
        "        faiss.write_index(merged_index, merged_index_path)\n",
        "\n",
        "        print(f\"Merged index saved to '{merged_index_path}'\")\n",
        "\n",
        "        del merged_index\n",
        "\n",
        "merged_index = faiss.IndexFlatL2(384)\n",
        "for idx, filename in enumerate(file_names):\n",
        "    if (idx + 1) <= 19:\n",
        "        continue\n",
        "\n",
        "    if (idx + 1) >= 12 and (idx + 1) <= 20:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder2, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    else:\n",
        "        indexname = f\"wikipedia_embeddings_collection_{idx + 1}_{filename}.index\"\n",
        "        print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "        index = faiss.read_index(os.path.join(index_folder1, indexname))\n",
        "\n",
        "        num_vectors = index.ntotal\n",
        "        for i in range(num_vectors):\n",
        "            vec = index.reconstruct(i).reshape(-1, 384)\n",
        "            vec = np.array(vec).astype(\"float32\")\n",
        "            merged_index.add(vec)\n",
        "\n",
        "    if (idx + 1) == 28:\n",
        "        merged_index_path = \"/kaggle/working/merged_4.index\"\n",
        "        faiss.write_index(merged_index, merged_index_path)\n",
        "\n",
        "        print(f\"Merged index saved to '{merged_index_path}'\")\n",
        "\n",
        "        del merged_index"
      ],
      "metadata": {
        "id": "nKfFO6G55Muk"
      },
      "id": "nKfFO6G55Muk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_index = faiss.IndexFlatL2(384)\n",
        "# merged_index = faiss.read_index(\"/kaggle/input/wikipedia-embeddings/merged_1.index\")\n",
        "index_folder = \"/kaggle/input/wikipedia-faiss-index\"\n",
        "\n",
        "for idx, indexname in enumerate(os.listdir(index_folder)):\n",
        "    print(f\"Merge file {idx + 1} - {indexname}\")\n",
        "    index = faiss.read_index(os.path.join(index_folder, indexname))\n",
        "\n",
        "    num_vectors = index.ntotal\n",
        "    for i in range(num_vectors):\n",
        "        vec = index.reconstruct(i).reshape(-1, 384)\n",
        "        vec = np.array(vec).astype(\"float32\")\n",
        "        merged_index.add(vec)\n",
        "\n",
        "    del index\n",
        "\n",
        "merged_index_path = \"/kaggle/working/merged.index\"\n",
        "faiss.write_index(merged_index, merged_index_path)\n",
        "\n",
        "print(f\"Merged index saved to '{merged_index_path}'\")"
      ],
      "metadata": {
        "id": "hCQpQREv5UPr"
      },
      "id": "hCQpQREv5UPr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V66Hhsit5U-E"
      },
      "id": "V66Hhsit5U-E",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}